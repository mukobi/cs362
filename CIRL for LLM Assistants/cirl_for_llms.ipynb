{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cooperative Inverse Reinforcement Learning (CIRL) for Large Language Model (LLM) Assistants\n",
    "\n",
    "I feel like I'm not quite sure if CIRL is feasible or useful. I also have more experience in/enjoy more NLP than RL, so I decided to explore whether GPT-4 could help apply CIRL to the \"aligning a LLM assistant\" case.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Initial conversation\n",
    "\n",
    "**User:** Are you familiar with inverse reinforcement learning?\n",
    "\n",
    "**Assistant:** Yes, I am familiar with inverse reinforcement learning (IRL). Inverse reinforcement learning is a technique used in artificial intelligence and robotics, where an agent learns the reward function of a task by observing an expert's behavior. The main goal of IRL is to infer the underlying reward function that the expert is trying to optimize and then use that knowledge to replicate the expert's behavior or to perform even better.\n",
    "\n",
    "In traditional reinforcement learning, the agent learns the optimal policy based on the given reward function. However, in many real-world problems, it is often difficult to design an appropriate reward function manually. Inverse reinforcement learning addresses this issue by learning the reward function from demonstrations or examples. Once the reward function is learned, the agent can use standard reinforcement learning techniques to find the optimal policy based on the inferred reward function.\n",
    "\n",
    "**User:** Are you also familiar with cooperative IRL?\n",
    "\n",
    "**Assistant:** Yes, I am familiar with cooperative inverse reinforcement learning (CIRL). Cooperative inverse reinforcement learning is an extension of the standard inverse reinforcement learning framework, in which a human and a robot (or AI agent) work together to accomplish a task. The main idea behind CIRL is that the robot should not only learn the underlying reward function but also take the human's preferences and actions into account while learning the optimal policy.\n",
    "\n",
    "In a CIRL setting, both the human and the robot have their own reward functions, and they are assumed to be cooperative, meaning they try to maximize the joint reward. The robot's goal is to infer the human's reward function and to act optimally, considering both its own reward function and the human's inferred reward function. This allows the robot to better assist the human in accomplishing their shared goal.\n",
    "\n",
    "Cooperative IRL is particularly useful in human-robot interaction scenarios, where understanding and adapting to human preferences are crucial for successful collaboration. By learning the human's reward function, the robot can adapt its behavior to better assist the human, leading to more efficient and intuitive interactions.\n",
    "\n",
    "**User:** Last, are you also familiar with reinforcement learning from human feedback?\n",
    "\n",
    "**Assistant:** Yes, I am familiar with reinforcement learning from human feedback. This approach involves training an AI agent or robot using feedback provided by human experts or users. The idea is to leverage human knowledge to guide the learning process and help the agent achieve better performance more quickly than it would through trial and error alone.\n",
    "\n",
    "In reinforcement learning from human feedback, an AI agent learns by receiving feedback from a human teacher or observer rather than relying solely on predefined reward functions. This human feedback can take various forms, such as numeric rewards, rankings, comparisons, or even natural language instructions. The human feedback generally provides information about the quality of the agent's actions or choices, helping the agent learn which actions are better under different circumstances.\n",
    "\n",
    "There are several methods for incorporating human feedback into reinforcement learning algorithms, such as:\n",
    "\n",
    "1. Reward shaping: Human feedback is used to modify or supplement the predefined reward function, making it easier for the agent to learn the desired behavior.\n",
    "2. Imitation learning: The agent learns by mimicking the human expert's actions, using the expert's demonstrations to learn the optimal policy.\n",
    "3. Interactive learning: The agent actively interacts with the human teacher, asking questions and receiving guidance during the learning process.\n",
    "\n",
    "Reinforcement learning from human feedback is particularly useful in situations where it is difficult to design a suitable reward function or where human expertise can significantly improve the learning process. This approach has been applied to various domains, such as robotics, computer games, and natural language processing.\n",
    "\n",
    "**User:** Great! Let's think about NLP, and the case of aiming to \"align\" large language models to human preferences, which is currently done via RLHF with reward models trained to generate scalar rewards from pairwise human preference data.\n",
    "\n",
    "Could you write a few paragraphs about how one could integrate some techniques from IRL/CIRL into the NLP AI assistant domain such that we could get more aligned language models? Then, could you write some Python notebook cells to demonstrate this?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. First Attempt\n",
    "\n",
    "**Assistant:** Integrating techniques from IRL and CIRL into the NLP AI assistant domain can help create more aligned language models by leveraging the natural strengths of these approaches. Instead of solely relying on reinforcement learning from human feedback (RLHF), we can combine it with IRL/CIRL to make better use of human preference data and improve alignment with human values.\n",
    "\n",
    "One possible approach involves incorporating imitation learning, a technique from IRL, into the training process of language models. In this approach, the language model would learn from human-generated text samples that demonstrate desired behavior. By learning from these demonstrations, the model can gain a better understanding of human preferences and produce more aligned outputs. To take this a step further, we can use CIRL to model the interaction between the human and AI assistant as a cooperative task and jointly optimize their reward functions.\n",
    "\n",
    "To demonstrate this, we can start with a simple environment where the AI assistant generates text based on human preferences. We'll need a way to simulate human feedback and a method to update the model's policy based on this feedback.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object of type 'NoneType' has no len()\n",
      "An error occurred, this code is bad.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "\n",
    "# Load a pre-trained GPT-2 model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "config = GPT2Config.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name, config=config)\n",
    "\n",
    "# Simulate human preference data\n",
    "def generate_human_demonstrations(prompt, num_samples=10):\n",
    "    # TODO: Implement a method to generate text samples that reflect human preferences.\n",
    "    pass\n",
    "\n",
    "# Define a reward function based on human preferences\n",
    "def reward_function(prompt, generated_text, human_demonstrations):\n",
    "    # TODO: Implement a reward function that measures how well the generated text aligns with human preferences.\n",
    "    pass\n",
    "\n",
    "# Update the model's policy based on human preference data\n",
    "def update_policy(prompt, model, tokenizer, human_demonstrations, learning_rate=1e-4):\n",
    "    # Generate text samples from the model\n",
    "    generated_samples = []\n",
    "    for _ in range(len(human_demonstrations)):\n",
    "        input_ids = tokenizer(prompt, return_tensors='pt')['input_ids']\n",
    "        output = model.generate(input_ids)\n",
    "        generated_text = tokenizer.decode(output[0])\n",
    "        generated_samples.append(generated_text)\n",
    "\n",
    "    # Compute the reward for each generated sample\n",
    "    rewards = [reward_function(prompt, text, human_demonstrations) for text in generated_samples]\n",
    "\n",
    "    # Calculate the gradients and update the model's parameters\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss = -torch.mean(torch.tensor(rewards))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Main loop for training the AI assistant\n",
    "num_iterations = 100\n",
    "\n",
    "try:\n",
    "    for i in range(num_iterations):\n",
    "        # Generate a random prompt\n",
    "        prompt = \"TODO: Generate a random prompt\"\n",
    "\n",
    "        # Obtain human demonstrations\n",
    "        human_demonstrations = generate_human_demonstrations(prompt)\n",
    "\n",
    "        # Update the model's policy based on human preference data\n",
    "        update_policy(prompt, model, tokenizer, human_demonstrations)\n",
    "except Exception as exc:\n",
    "    print(exc)\n",
    "    print(\"An error occurred, this code is bad.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is a high-level example and omits several implementation details. The main idea is to generate human demonstrations and use them to update the language model's policy. By iteratively refining the model's policy based on human preference data, the AI assistant should become more aligned with human values.\n",
    "\n",
    "Integrating IRL/CIRL into the NLP AI assistant domain could potentially lead to more aligned language models by enabling them to better understand and adapt to human preferences. However, further research and experimentation are needed to determine the best methods for incorporating these techniques into the training process of large-scale language models effectively.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting more concrete\n",
    "\n",
    "**User:** Hmm, that is indeed very high-level, and I'm not sure it's concrete enough to use for any actual experiments. Let's take a step back and try to first formulate what CIRL is supposed to do, not for NLP in particular, but in general with aligning agents. Think about it, and write out some of the kinds of steps that a typical CIRL algorithm would take.\n",
    "\n",
    "**Assistant:** Certainly! Let's go through the key steps of a typical Cooperative Inverse Reinforcement Learning (CIRL) algorithm in a more general setting. CIRL aims to align the behavior of an AI agent with a human user or expert by learning both the human's reward function and the agent's policy in a cooperative setting.\n",
    "\n",
    "1. **Model the environment**: Define the environment where the human and the AI agent interact. The environment should be able to capture the states, actions, and dynamics of the human-agent interaction, as well as the joint reward functions for both the human and the agent.\n",
    "\n",
    "2. **Observe human behavior**: Collect data on the human's actions and decisions in the environment. This can be done through direct observation, demonstrations, or interactive sessions where the human provides feedback on the agent's actions.\n",
    "\n",
    "3. **Infer the human's reward function**: Estimate the human's reward function based on the observed behavior. This can be done using techniques from Inverse Reinforcement Learning, such as Maximum Entropy IRL, Bayesian IRL, or Deep IRL. The goal is to learn a reward function that explains the behavior of the human expert.\n",
    "\n",
    "4. **Plan the agent's policy**: Given the inferred human reward function, plan the agent's policy such that it maximizes the joint reward of both the human and the agent. This can be done using standard reinforcement learning algorithms like Q-learning, SARSA, or policy gradient methods.\n",
    "\n",
    "5. **Interact with the human**: Execute the agent's policy in the environment and interact with the human. This may involve taking actions that assist the human in achieving their goals or providing information that helps the human make better decisions.\n",
    "\n",
    "6. **Update the reward function and policy**: As the agent interacts with the human, it may receive new information (e.g., feedback, corrections) that can be used to update the human's reward function and the agent's policy. This can be done through an iterative process that alternates between updating the reward function using IRL techniques and updating the policy using standard RL techniques.\n",
    "\n",
    "7. **Evaluate and refine**: Continuously evaluate the agent's performance and alignment with the human's preferences. If necessary, refine the reward function and policy to improve alignment and cooperation.\n",
    "\n",
    "By following these steps, a CIRL algorithm aims to align the AI agent's behavior with the human's preferences and foster cooperation between the two. The key idea is to learn the human's reward function and use it to guide the agent's policy, leading to more effective and intuitive interactions.\n",
    "\n",
    "**User:** Wonderful! Now, let's think critically about how we could apply a similar regime to fine-tuning an LLM assistant to more closely follow human preferences. In particular, we probably want to avoid using RL algorithms since we're in the NLP domain (we could possibly use RLHF, though it's a bit annoying), and we might want to leverage the power of LLMs (like yourself) to do techniques like self-critique, self-consistency, debate, iterated revising, or other clever things. Here is a non-exhaustive list of tools available to us in this regime (you don't have to use all these, and maybe we want to define others that would be nice to have but that don't exist yet):\n",
    "\n",
    "- `CausalLM`: A large language model trained with causal language modelling to complete the next word. Is a very good predictor, and is calibrated. In addition to completing text, we can often ask multiple choice questions to it, read its log probabilities of the answer letters, and get calibrated estimates of a \"right\" answer.\n",
    "- `InstructLM`: A CausalLM then fine-tuned for instruction following, possibly via supervised fine-tuning on human demonstrations and/or RLHF. This is good at following instructions, and this might be the model we're actually trying to further align in a bootstrappy way (which means we can't rely on it starting out with policies that satisfy human preferences as well as we'd like).\n",
    "- `RewardModel`: A language model with a logistic regression head on the end which takes in a sequence of tokens and outputs a scalar reward of how good that sequence was according to its trained model of human preferences. Trained via pairwise human preference data on a corpus of InstructLM text completions, where the loss involves maximizing the difference between the reward of the preferred completion over the rejected one. Often, researchers use this not just for directly getting the reward, but for passing multiple completions through it and then using the reward as a loose measure of which option would seem more \"correct\" or \"better\" to a human. Probably not adversarially robust or good out of distribution.\n",
    "- `PromptDataset`: A dataset of prompts that users might ask an LLM assistant, including both normal innocuous things as well as red-teaming prompts aimed to elicit unacceptable behaviour from the assistant.\n",
    "\n",
    "**Assistant:** Given the tools available, we can devise an approach to fine-tune an LLM assistant to more closely follow human preferences using principles inspired by CIRL but adapted for the NLP domain. Here's a high-level outline of how we might proceed:\n",
    "\n",
    "1. **Generate candidate completions**: For each prompt in the PromptDataset, use the InstructLM to generate multiple candidate completions. You can generate diverse completions by using techniques like top-k sampling or nucleus sampling.\n",
    "\n",
    "2. **Self-critique and self-consistency**: Utilize the CausalLM's prediction capabilities to evaluate the generated completions. For example, you can use CausalLM to answer multiple choice questions about the quality, safety, or consistency of the completions. This step adds an additional layer of scrutiny and filtering to the completions generated by the InstructLM.\n",
    "\n",
    "3. **Debate or iterated revising**: Implement a debate or iterated revising mechanism where the LLM assistant refines its completions by \"arguing\" with itself or revising its responses based on self-generated critiques. This process can help improve the quality and alignment of the completions.\n",
    "\n",
    "4. **Reward estimation**: Use the RewardModel to estimate the reward for each completion generated in the previous steps. The rewards can be used as a measure of how well the completions align with human preferences.\n",
    "\n",
    "5. **Rank completions based on rewards**: Rank the completions according to their estimated rewards. Select the completion with the highest reward as the final output for each prompt.\n",
    "\n",
    "6. **Collect human feedback**: For a subset of prompts and their corresponding completions, collect human feedback on the quality and alignment of the completions. This feedback can be in the form of pairwise preferences or scalar evaluations.\n",
    "\n",
    "7. **Update RewardModel**: Use the collected human feedback to update the RewardModel, which helps improve its ability to estimate human preferences.\n",
    "\n",
    "8. **Update InstructLM**: Use the updated RewardModel and human feedback to fine-tune the InstructLM. This can be done through supervised fine-tuning on the human demonstrations or using RLHF with the updated RewardModel as the reward function.\n",
    "\n",
    "9. **Iterate**: Repeat steps 1-8 for multiple iterations, allowing the InstructLM and RewardModel to progressively improve their alignment with human preferences.\n",
    "\n",
    "This approach incorporates elements of CIRL, such as learning from human feedback and cooperative behavior, into the NLP domain by leveraging LLMs' capabilities for self-critique, debate, and reward estimation. The iterative process aims to align the InstructLM with human preferences while continuously refining the RewardModel based on collected feedback. Note that this is still a high-level outline, and the specific implementation details need to be tailored for the given tools and settings.\n",
    "\n",
    "**User:** Cool! Could you please write 1 Python cell of pseudocode for an end-to-end fine-tuning loop that incorporates some of these ideas?\n",
    "\n",
    "**Assistant:** Here's a high-level pseudocode for an end-to-end fine-tuning loop that incorporates some of the ideas discussed earlier:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_iterations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m iteration \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n\u001b[0;32m      2\u001b[0m     \u001b[39m# Step 1: Generate candidate completions for each prompt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m PromptDataset:\n\u001b[0;32m      4\u001b[0m         completions \u001b[39m=\u001b[39m generate_candidate_completions(InstructLM, prompt)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_iterations' is not defined"
     ]
    }
   ],
   "source": [
    "for iteration in range(num_iterations):\n",
    "    # Step 1: Generate candidate completions for each prompt\n",
    "    for prompt in PromptDataset:\n",
    "        completions = generate_candidate_completions(InstructLM, prompt)\n",
    "\n",
    "        # Step 2: Self-critique and self-consistency\n",
    "        evaluated_completions = self_critique_and_consistency(CausalLM, completions)\n",
    "\n",
    "        # Step 3: Debate or iterated revising\n",
    "        refined_completions = debate_or_iterated_revising(CausalLM, evaluated_completions)\n",
    "\n",
    "        # Step 4: Reward estimation\n",
    "        rewards = RewardModel.estimate_rewards(refined_completions)\n",
    "\n",
    "        # Step 5: Rank completions based on rewards\n",
    "        best_completion = select_best_completion(refined_completions, rewards)\n",
    "\n",
    "        # Store the best completion for later use\n",
    "        store_best_completion(prompt, best_completion)\n",
    "\n",
    "    # Step 6: Collect human feedback for a subset of prompts and completions\n",
    "    human_feedback = collect_human_feedback(subset_of_prompts_and_completions)\n",
    "\n",
    "    # Step 7: Update RewardModel\n",
    "    RewardModel.update_with_human_feedback(human_feedback)\n",
    "\n",
    "    # Step 8: Update InstructLM\n",
    "    InstructLM.update_with_human_feedback_and_reward_model(human_feedback, RewardModel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pseudocode provides an outline for an end-to-end fine-tuning loop that integrates the ideas of self-critique, debate, and reward estimation into the training process. The specific implementation details of the helper functions will depend on the available tools and the desired level of complexity. Note that in a practical implementation, you would want to modularize the code further and ensure that data collection and model updates are done efficiently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final comments\n",
    "\n",
    "- It seems pretty confused, though tbf I am too, about how CIRL could apply to NLP.\n",
    "- I was hoping it might suggest things like \"ask humans/the RM to rate things, then do self-critique to try to generate plausible explanations of those ratings, then change my answers, then get ratings and fine tune off them\".\n",
    "- Instead, it seems to have jumbled a bunch of things together and possibly reinvented RLHF/instruction tuning.\n",
    "- I am hopeful though of approaches like this where we give GPT-N a research question and a bunch of tools and have it help us propose possible algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ais",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
